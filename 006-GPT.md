[GPT](https://platform.openai.com/docs/guides/gpt/gpt)



寻找 ChatGPT 吗？请前往[chat.openai.com](https://chat.openai.com/)。

OpenAI 的 GPT（生成式预训练转换器）模型已经训练过，可以理解自然语言和代码。GPTs 会以文本形式回应输入。GPTs 的输入也被称为“提示”。设计提示本质上就是如何“编程” GPT 模型，通常是通过提供说明或一些成功完成任务的操作示例。

使用 GPTs，您可以构建以下应用程序：

- 起草文件
- 编写计算机代码
- 对知识库提取问答
- 分析文本
- 创建会话代理（聊天机器人）
- 为软件提供自然语言界面
- 在各个学科领域提供辅导
- 翻译语言
- 模拟游戏角色
- 还有很多其他功能。

要通过 OpenAI API 使用 GPT 模型，您需要发送一个包含输入和 API 密钥的请求，并接收包含模型输出的响应。我们最新的模型“gpt-4”和“gpt-3.5-turbo”可以通过聊天完成 API 终点进行访问。目前，只有较旧的遗留模型可以通过完成 API 终点进行访问。

|                          | 模型系列                                                     | API 终点                                   |
| :----------------------- | :----------------------------------------------------------- | :----------------------------------------- |
| 更新的模型（2023-）      | gpt-4, gpt-3.5-turbo                                         | https://api.openai.com/v1/chat/completions |
| 旧模型（2020年至2022年） | text-davinci-003, text-davinci-002, davinci, curie, babbage, ada | https://api.openai.com/v1/completions      |

你可以在[playground](https://platform.openai.com/playground?mode=chat)上尝试GPTs。如果不确定要使用哪个模型，可以使用 `gpt-4` 或 `gpt-3.5-turbo`。

[聊天完成API](https://platform.openai.com/docs/guides/gpt/chat-completions-api)

聊天模型将消息列表作为输入，返回模型生成的消息作为输出。虽然聊天格式旨在使多轮对话更加容易，但在没有任何对话的单轮任务中使用它同样有用。

一个示例API调用如下：

```python
import openai

openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```

查看完整的API参考文档[在这里](https://platform.openai.com/docs/api-reference/chat)。

主要的输入参数是messages参数。 messages必须是一组消息对象的数组，每个对象都有一个角色（系统，用户或助手）和内容。对话可以只包含一条消息，也可以包含多轮来回的对话。

通常，一次对话的格式以系统消息开头，随后是交替出现的用户和助手消息。

系统消息有助于设置助手的行为。例如，您可以修改助手的个性或提供有关在整个对话中它应该如何运作的特定指示。请注意，系统消息是可选的，而没有系统消息的模型行为很可能类似于使用通用消息，例如“您是一位有帮助的助手”。

用户消息提供请求或评论，让助手做出回应。助手消息存储先前的助手响应，但开发人员也可以编写助手消息来提供所需的行为示例。

包括对话历史记录在内对于用户指令所涉及的以前消息是很重要的。在上面的例子中，用户的最后一个问题“在哪里播放？”只有在关于2020世界大赛的先前消息的背景下才有意义。由于模型没有记录过去的请求，所有相关信息必须作为每个请求的对话历史的一部分提供。如果对话无法适应模型的记号限制，那么它将需要以某种方式进行缩短。例如可以 [进行简短的总结或筛选前面的对话](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-for-dialogue-applications-that-require-very-long-conversations-summarize-or-filter-previous-dialogue)。

要模仿ChatGPT中出现的文本逐步返回的效果，将[stream](https://platform.openai.com/docs/api-reference/chat/create#chat/create-stream)参数设置为true。

[聊天完成响应格式](https://platform.openai.com/docs/guides/gpt/chat-completions-response-format)

聊天完成API响应的示例如下：

```text
{
 'id': 'chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve',
 'object': 'chat.completion',
 'created': 1677649420,
 'model': 'gpt-3.5-turbo',
 'usage': {'prompt_tokens': 56, 'completion_tokens': 31, 'total_tokens': 87},
 'choices': [
   {
    'message': {
      'role': 'assistant',
      'content': 'The 2020 World Series was played in Arlington, Texas at the Globe Life Field, which was the new home stadium for the Texas Rangers.'},
    'finish_reason': 'stop',
    'index': 0
   }
  ]
}
```

在Python中，可以使用`response['choices'][0]['message']['content']`来提取助手的回复。

每个响应都包括一个`finish_reason`字段。`finish_reason`可能的值包括：

- `stop`: API 返回了完整的模型输出
- `length`: 模型输出不完整，由于 [`max_tokens`](https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens) 参数或标记限制
- `content_filter`: 由于我们的内容过滤器的标志，忽略了某些内容
- `null`: API响应仍在进行中或不完整

[完成API](https://platform.openai.com/docs/guides/gpt/completions-api)

完成API终点的接口与聊天完成终点不同。输入不是消息列表，而是一个名为`prompt`的自由格式文本字符串。

下面是一个API调用的示例：

```python
import openai

response = openai.Completion.create(
  model="text-davinci-003",
  prompt="Write a tagline for an ice cream shop."
)
```

详细了解请参阅[API参考文档](https://platform.openai.com/docs/api-reference/completions)。

[Token日志概率](https://platform.openai.com/docs/guides/gpt/token-log-probabilities)

完成API可以提供与每个输出令牌相关的最可能令牌的有限数量的对数概率。该功能由使用[logprobs](https://platform.openai.com/docs/api-reference/completions/create#completions/create-logprobs)字段来控制。在某些情况下，这可以有助于评估模型在输出方面的自信心。

[插入文本](https://platform.openai.com/docs/guides/gpt/inserting-text)

完成终端还支持通过提供[后缀](https://platform.openai.com/docs/api-reference/completions/create#completions/create-suffix)来插入文本，除了标准提示作为前缀外。在撰写长篇文本、段落之间的转换、遵循大纲或引导模型走向结束时，自然需要这样做。它还适用于代码，并可用于在函数或文件中间插入文本。

深入挖掘

插入文本



为了说明后缀上下文如何影响生成的文本，考虑提示语句“I decided to make a big change.（今天我决定做出重大改变。）”。有许多种方式可以想象完成这个句子。但如果我们现在提供了故事的结尾：“I’ve gotten many compliments on my new hair!”（我新的发型受到了很多赞美！），那么预期的完成就变得清晰了。

> 我在波士顿大学上大学。拿到学位后，我决定做出改变。一个重大的决定！我收拾行李，搬到了美国西海岸。现在，我再也离不开太平洋了！

通过为该模型提供额外的上下文，我们可以更加精确地引导模型的生成结果，但这对于模型来说是更受限制和有挑战性的任务。为了获得最佳结果，我们建议采取以下策略：

**使用max_tokens >256。** 模型在插入更长的完成部分时更为出色。如果使用太小的max_tokens，则模型可能在连接后缀之前被截断。请注意，即使使用较大的max_tokens，您也只会因生成的token数量而产生费用。

**最好使用finish_reason == "stop"。** 当模型到达自然停顿点或用户提供的停止序列时，模型将将finish_reason设置为"stop"。这表明模型已成功连接到后缀，是一个完成品质量的良好信号。这对于在使用n>1或重新采样（见下一个提示）时在几个完成品之间进行选择尤其重要。

**重新采样3-5次。虽然几乎所有完成品都能连接到前缀，但在更困难的情况下，模型可能会难以连接后缀。我们发现，在这种情况下，重新采样3或5次（或使用k=3、5的best_of）并选择带有"stop"作为finish_reason的样本可以是一种有效的方法。在重新采样时，您通常想要使用较高的温度来增加多样性。

请注意：如果所有返回的样本都有finish_reason=="length"，那么max_tokens可能太小了，在模型能够自然地连接提示和后缀之前，模型的tokens可能就已用完了。考虑在重新采样之前增加max_tokens。

尝试提供更多的线索。在一些情况下，为了更好地帮助模型生成，您可以通过提供一些模型可以遵循的模式示例来提供线索，以决定自然停止的位置。



如何制作美味的热巧克力：

\1. **煮沸水**

**2. 在杯子里放入热巧克力**

**3. 将沸水倒在杯子里**

\4. 尽情享受美味的热巧克力。



\1. 狗是忠诚的动物。

\2. 狮子是凶猛的动物。

\3. 海豚**是爱玩的动物。**

\4. 马是威严的动物。



[完成响应格式

一个完成API的响应示例如下所示：

```text
{
  "choices": [
    {
      "finish_reason": "length",
      "index": 0,
      "logprobs": null,
      "text": "\n\n\"Let Your Sweet Tooth Run Wild at Our Creamy Ice Cream Shack"
    }
  ],
  "created": 1683130927,
  "id": "cmpl-7C9Wxi9Du4j1lQjdjhxBlO22M61LD",
  "model": "text-davinci-003",
  "object": "text_completion",
  "usage": {
    "completion_tokens": 16,
    "prompt_tokens": 10,
    "total_tokens": 26
  }
}
```

在 Python 中，可以使用 `response['choices'][0]['text']` 提取输出。

响应格式类似于聊天完成 API 的响应格式，但还包括可选字段 `logprobs`。

[聊天完成 vs 完成](https://platform.openai.com/docs/guides/gpt/chat-completions-vs-completions)

聊天完成格式可以通过使用单个用户消息构建请求来与完成格式类似。例如，可以使用以下完成提示将英语翻译成法语：

```text
Translate the following English text to French: "{text}"
```

类似的聊天提示可以是：



```text
[{"role": "user", "content": 'Translate the following English text to French: "{text}"'}]
```

同样，完成 API 可以通过相应的输入格式来模拟用户和助手之间的聊天，详情请参见[此链接](https://platform.openai.com/playground/p/default-chat?model=text-davinci-003)。

这些 API 的区别主要来自于它们所使用的基础 GPT 模型。聊天完成 API 是我们最强大的模型 (`gpt-4`) 和我们最具成本效益的模型 (`gpt-3.5-turbo`) 的接口。以 `gpt-3.5-turbo` 为例，其表现水平类似于 `text-davinci-003`，但每个标记的价格只有后者的 10%！有关定价详细信息，请参见[此处](https://openai.com/pricing)。

[我应该使用哪个模型？](https://platform.openai.com/docs/guides/gpt/which-model-should-i-use)

我们通常建议开发者使用 `gpt-4` 或 `gpt-3.5-turbo` 中的一种。你应该使用哪个模型取决于你使用模型的任务的复杂性。`gpt-4` 在广泛的[评估](https://arxiv.org/abs/2303.08774)中表现更好。特别是，`gpt-4` 更能够仔细地遵循复杂的指令。相比之下，`gpt-3.5-turbo` 更有可能仅遵循复杂多部分指令的一部分。`gpt-4` 比 `gpt-3.5-turbo` 更不容易虚构信息，这种行为被称为“幻觉”。`gpt-4` 的上下文窗口也更大，最大大小为 8,192 个标记，而 `gpt-3.5-turbo` 的大小为 4,096 个标记。然而，`gpt-3.5-turbo` 返回的输出具有更低的延迟，并且每个标记的成本要少得多。

我们建议在 [playground](https://platform.openai.com/playground?mode=chat) 中实验，以确定哪些模型提供了最佳的价格性能权衡。一种常见的设计模式是使用一些不同类型的查询，每个查询类型都被分派到适当处理它们的模型中。

[GPT 最佳实践](https://platform.openai.com/docs/guides/gpt/gpt-best-practices)

了解使用 GPT 的最佳实践对应用程序的性能会产生显着影响。GPT 展示的失败模式以及解决或纠正这些失败模式的方法并不总是直观的。在使用 GPT 时需要一种名为“提示工程”的技能，但随着该领域的发展，其范围已经超越了仅工程调整提示，而是转向了将模型查询用作组件的工程系统。要了解更多信息，请阅读我们的 [GPT 最佳实践指南](https://platform.openai.com/docs/guides/gpt-best-practices)，其中包括提高模型推理、减少模型幻觉等方法。您还可以在 [OpenAI Cookbook](https://github.com/openai/openai-cookbook) 中找到许多有用的资源，包括代码示例。

[管理令牌](https://platform.openai.com/docs/guides/gpt/managing-tokens)

语言模型读取和写入称为令牌的文本块。在英语中，令牌可以是一个字符或一个单词（例如，“a”或“apple”），在某些语言中，令牌甚至可以比一个字符更短，或者比一个单词更长。

例如，字符串“ChatGPT is great！”被编码为六个令牌：`["Chat", "G", "PT", " is", " great", "!"]`。

API 调用中的令牌总数影响以下几个方面：

- 您的 API 调用费用，因为您需要按令牌付费；
- 您的 API 调用时间长度，因为写入更多令牌需要更多时间；
- 您的 API 调用是否起作用，因为总令牌数量必须低于模型的最大限制（对于 “gpt-3.5-turbo” 是 4096 个令牌）。

输入和输出令牌都计入这些数量。例如，如果您的 API 调用在消息输入中使用了 10 个令牌，并在消息输出中收到了 20 个令牌，您将被收取 30 个令牌的费用。但需要注意的是，对于某些模型，输入与输出令牌的单价是不同的（请参阅[定价](https://openai.com/pricing)页面以获取更多信息）。

要查看 API 调用使用了多少令牌，请检查 API 响应中的 `usage` 字段（例如，`response['usage']['total_tokens']`）。

像`gpt-3.5-turbo`和 `gpt-4` 这样的聊天模型和 completions API 中的模型使用相同的方式计数令牌，但由于它们基于消息格式，因此更难确定一个对话将使用多少令牌。

深入探究

计算聊天 API 调用中的令牌数



下面是一个示例函数，用于计算传递给 gpt-3.5-turbo-0301 的消息中的令牌数。

从一个模型转换成令牌的方法可能因模型而异。因此，当未来的模型版本发布时，此函数返回的答案可能仅近似。[ChatML文档](https://github.com/openai/openai-python/blob/main/chatml.md)解释了如何将消息转换为 OpenAI API 识别的令牌，并可能有用于编写自己的函数。

```python
def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
  """Returns the number of tokens used by a list of messages."""
  try:
      encoding = tiktoken.encoding_for_model(model)
  except KeyError:
      encoding = tiktoken.get_encoding("cl100k_base")
  if model == "gpt-3.5-turbo-0301":  # note: future models may deviate from this
      num_tokens = 0
      for message in messages:
          num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
          for key, value in message.items():
              num_tokens += len(encoding.encode(value))
              if key == "name":  # if there's a name, the role is omitted
                  num_tokens += -1  # role is always required and always 1 token
      num_tokens += 2  # every reply is primed with <im_start>assistant
      return num_tokens
  else:
      raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.
  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")
```

接下来，创建一个消息并将其传递到上面定义的函数中以查看令牌计数，这应该与 API 使用参数返回的值相匹配：

```python
messages = [
  {"role": "system", "content": "You are a helpful, pattern-following assistant that translates corporate jargon into plain English."},
  {"role": "system", "name":"example_user", "content": "New synergies will help drive top-line growth."},
  {"role": "system", "name": "example_assistant", "content": "Things working well together will increase revenue."},
  {"role": "system", "name":"example_user", "content": "Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage."},
  {"role": "system", "name": "example_assistant", "content": "Let's talk later when we're less busy about how to do better."},
  {"role": "user", "content": "This late pivot means we don't have time to boil the ocean for the client deliverable."},
]

model = "gpt-3.5-turbo-0301"

print(f"{num_tokens_from_messages(messages, model)} prompt tokens counted.")
# Should show ~126 total_tokens
```

为了确认我们上面的函数生成的数字与 API 返回的数字一致，请创建一个新的 Chat Completion：

```python
# example token count from the OpenAI API
import openai


response = openai.ChatCompletion.create(
    model=model,
    messages=messages,
    temperature=0,
)

print(f'{response["usage"]["prompt_tokens"]} prompt tokens used.')
```

要在不进行 API 调用的情况下查看文本字符串中有多少个令牌，请使用 OpenAI 的 [tiktoken](https://github.com/openai/tiktoken) Python 库。示例代码可以在 OpenAI Cookbook 的 [如何使用 tiktoken 计算令牌数的指南](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) 中找到。

每个传递给 API 的消息都会消耗内容、角色和其他字段中的令牌数量，再加上一些用于幕后格式化的额外令牌。这可能会在将来略有改变。

如果一个对话的令牌数超过了模型的最大限制（例如，gpt-3.5-turbo 的令牌数超过了4096个），你将不得不缩短或省略你的文本，直到它适合模型的要求。要注意，如果从 messages 输入中删除了一条消息，模型将丧失所有与该消息相关的知识。

请注意，很长的对话更有可能收到不完整的回复。例如，一个长度为4090个令牌的 gpt-3.5-turbo 对话，在回复的前6个令牌后就会被截断。

[FAQ（常见问题解答）](https://platform.openai.com/docs/guides/gpt/faq)

[为什么模型输出不一致？](https://platform.openai.com/docs/guides/gpt/why-are-model-outputs-inconsistent)

这个 API 默认是不确定性的。这意味着即使你的提示（prompt）保持不变，每次调用时都可能得到略有不同的输出结果。将温度参数设置为0可以使输出结果大部分确定性，但仍会留下一小部分变化。

[我应该如何设置温度参数？](https://platform.openai.com/docs/guides/gpt/how-should-i-set-the-temperature-parameter)

温度参数的较低值会产生更一致的结果，而较高的值会产生更多样化和创造性的结果。根据你的特定应用程序所需要的连贯性和创造性的权衡来选择温度值。

[最新模型支持微调吗？](https://platform.openai.com/docs/guides/gpt/is-fine-tuning-available-for-the-latest-models)

不支持。目前，你只能微调基础的 GPT-3 模型（包括 `davinci`、`curie`、`babbage` 和 `ada`）。查看 [微调指南](https://platform.openai.com/docs/guides/fine-tuning) 以获取有关如何使用微调模型的更多详细信息。

[你们会存储传递到 API 的数据吗？](https://platform.openai.com/docs/guides/gpt/do-you-store-the-data-that-is-passed-into-the-api)

截至 2023 年 3 月 1 日，我们将保留您的 API 数据 30 天，但不再使用通过 API 发送的数据来改进我们的模型。了解更多信息，请参阅我们的[数据使用政策](https://openai.com/policies/usage-policies)。

[添加审查层](https://platform.openai.com/docs/guides/gpt/adding-a-moderation-layer)

如果你想要在聊天 API 的输出结果中添加一个审查层，可以按照我们的[审查指南](https://platform.openai.com/docs/guides/moderation)来防止显示违反 OpenAI 使用政策的内容。

[我应该使用 ChatGPT 还是 API？](https://platform.openai.com/docs/guides/gpt/should-i-use-chatgpt-or-the-api)

[ChatGPT](https://platform.openai.com/docs/guides/gpt/chat.openai.com)提供了一个与 OpenAI API 模型进行交互的聊天界面和一系列内置功能，如集成浏览、代码执行、插件等。相比之下，使用 OpenAI 的 API 为开发者提供了更多的灵活性。

